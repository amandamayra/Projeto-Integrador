{
	"jobConfig": {
		"name": "data_quality_checks",
		"description": "",
		"role": "arn:aws:iam::302526197261:role/LabRole",
		"command": "glueetl",
		"version": "5.0",
		"runtime": null,
		"workerType": "G.1X",
		"numberOfWorkers": 10,
		"maxCapacity": 10,
		"jobRunQueuingEnabled": false,
		"maxRetries": 0,
		"timeout": 480,
		"maxConcurrentRuns": 1,
		"security": "none",
		"scriptName": "data_quality_checks.py",
		"scriptLocation": "s3://aws-glue-assets-302526197261-us-east-1/scripts/",
		"language": "python-3",
		"spark": true,
		"sparkConfiguration": "standard",
		"jobParameters": [],
		"tags": [],
		"jobMode": "DEVELOPER_MODE",
		"createdOn": "2025-04-10T00:56:31.728Z",
		"developerMode": true,
		"connectionsList": [],
		"temporaryDirectory": "s3://aws-glue-assets-302526197261-us-east-1/temporary/",
		"logging": true,
		"glueHiveMetastore": true,
		"etlAutoTuning": true,
		"metrics": true,
		"observabilityMetrics": true,
		"bookmark": "job-bookmark-disable",
		"sparkPath": "s3://aws-glue-assets-302526197261-us-east-1/sparkHistoryLogs/",
		"flexExecution": false,
		"minFlexWorkers": null,
		"maintenanceWindow": null
	},
	"hasBeenSaved": false,
	"usageProfileName": null,
	"script": "import sys\r\nfrom pyspark.context import SparkContext\r\nfrom awsglue.context import GlueContext\r\nfrom awsglue.utils import getResolvedOptions\r\nfrom awsglue.job import Job\r\nfrom pyspark.sql.functions import col, count, when, isnan, lit, current_timestamp\r\nfrom pyspark.sql.types import StructType, StructField, StringType\r\n\r\n# nicialização do glue\r\nargs = getResolvedOptions(sys.argv, ['JOB_NAME'])\r\nsc = SparkContext()\r\nglueContext = GlueContext(sc)\r\nspark = glueContext.spark_session\r\njob = Job(glueContext)\r\njob.init(args['JOB_NAME'], args)\r\n\r\n# caminhos dos buckets\r\ntrusted_base_path = \"s3://health-insurance-trusted/\"\r\ndelivery_path = \"s3://health-insurance-delivery/data_quality_checks/\"\r\n\r\n# tabelas e colunas-chave por dimensão\r\ntabelas_config = {\r\n    \"Rate.csv\": {\r\n        \"path\": f\"{trusted_base_path}Rate.csv/\",\r\n        \"completude\": [\"PlanId\", \"StateCode\", \"Age\", \"IndividualRate\"],\r\n        \"acuracia\": [(\"IndividualRate\", lambda df: df.filter((col(\"IndividualRate\") <= 0) | col(\"IndividualRate\").isNull()).count())],\r\n        \"consistencia\": [(\"PlanId\", \"BenefitsCostSharing.csv\")]  # Deve existir em outra tabela\r\n    },\r\n    \"BenefitsCostSharing.csv\": {\r\n        \"path\": f\"{trusted_base_path}BenefitsCostSharing.csv/\",\r\n        \"completude\": [\"PlanId\", \"BenefitName\", \"IsCovered\"],\r\n    },\r\n    \"PlanAttributes.csv\": {\r\n        \"path\": f\"{trusted_base_path}PlanAttributes.csv/\",\r\n        \"completude\": [\"PlanId\", \"StateCode\", \"MetalLevel\"],\r\n    }\r\n\r\n}\r\n\r\n# lista para resultados\r\nchecks = []\r\n\r\n# função para aplicar verificações de qualidade\r\ndef aplicar_checks(tabela_nome, config):\r\n    df = spark.read.option(\"header\", \"true\").csv(config[\"path\"])\r\n    total = df.count()\r\n\r\n    # completude\r\n    for col_name in config.get(\"completude\", []):\r\n        nulos = df.filter(col(col_name).isNull() | (col(col_name) == \"\")).count()\r\n        porcentagem_nula = (nulos / total) * 100 if total > 0 else 0\r\n        checks.append((tabela_nome, col_name, \"completude\", f\"{porcentagem_nula:.2f}%\", \"OK\" if nulos == 0 else \"FALHA\"))\r\n\r\n    # acurácia\r\n    for col_name, func in config.get(\"acuracia\", []):\r\n        erros = func(df)\r\n        checks.append((tabela_nome, col_name, \"acurácia\", f\"{erros} registros inválidos\", \"OK\" if erros == 0 else \"FALHA\"))\r\n\r\n    # consistência (verifica existência cruzada de chaves)\r\n    for col_name, ref_table in config.get(\"consistencia\", []):\r\n        df_ref = spark.read.option(\"header\", \"true\").csv(f\"{trusted_base_path}{ref_table}\")\r\n        ref_keys = df_ref.select(col_name).distinct()\r\n        origem_keys = df.select(col_name).distinct()\r\n        faltantes = origem_keys.join(ref_keys, col_name, \"left_anti\").count()\r\n        checks.append((tabela_nome, col_name, \"consistência\", f\"{faltantes} valores não encontrados em {ref_table}\", \"OK\" if faltantes == 0 else \"FALHA\"))\r\n\r\n# executa os checks para todas as tabelas configuradas\r\nfor tabela, cfg in tabelas_config.items():\r\n    aplicar_checks(tabela, cfg)\r\n\r\n# criação do DataFrame com resultados\r\nschema = StructType([\r\n    StructField(\"Tabela\", StringType(), True),\r\n    StructField(\"Campo\", StringType(), True),\r\n    StructField(\"Dimensao\", StringType(), True),\r\n    StructField(\"Descricao\", StringType(), True),\r\n    StructField(\"Status\", StringType(), True),\r\n])\r\ndf_resultados = spark.createDataFrame(checks, schema) \\\r\n    .withColumn(\"TimestampExecucao\", current_timestamp()) \\\r\n    .withColumn(\"JobName\", lit(args[\"JOB_NAME\"]))\r\n\r\n# escrita na camada delivery\r\n#df_resultados.write.mode(\"overwrite\").option(\"header\", \"true\").csv(delivery_path)\r\ndf_resultados.coalesce(1) \\\r\n             .write.mode(\"overwrite\") \\\r\n             .option(\"header\", \"true\") \\\r\n             .csv(delivery_path)\r\n\r\njob.commit()"
}